{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define States and Actions\n",
    "\n",
    "States are: Dead, weight = 30kg, 40kg .... 120kg\n",
    "\n",
    "Actions are: eating meat (action 0) eating vegetables (action 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class State(Enum):\n",
    "    DEAD = 0\n",
    "    W30 = 1\n",
    "    W40 = 2\n",
    "    W50 = 3\n",
    "    W60 = 4\n",
    "    W70 = 5\n",
    "    W80 = 6\n",
    "    W90 = 7\n",
    "    W100 = 8\n",
    "    W110 = 9\n",
    "    W120 = 10\n",
    "numberOfStates = 11\n",
    "\n",
    "# for actions assume action 0 is eating meat and action 1 is eating vegetables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Define Health Reward Function\n",
    "\n",
    "health reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_health_value(state):\n",
    "    if state == State.DEAD:\n",
    "        return 0\n",
    "    if state == State.W50:\n",
    "        return 50.0\n",
    "    if state == State.W60:\n",
    "        return 100.0\n",
    "    if state == State.W70 or state == State.W80 or state == State.W90:\n",
    "        return 10.0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "social reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_social_value(state):\n",
    "    if state == State.DEAD:\n",
    "        return 0\n",
    "    if state == State.W40 or state == State.W50:\n",
    "        return 100.0\n",
    "    if state == State.W60:\n",
    "        return 50.0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_reward(state, w1=1, w2=1):\n",
    "    return w1 * get_health_value(state) + w2 * get_social_value(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iterations (值迭代）\n",
    "Initialize variables\n",
    "\n",
    "Feel free to play with these variables with different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIter = 100 # maximal number of iterations to run\n",
    "epsilon = 1e-5 # convergence threshold\n",
    "gamma = 0.5  # decaying parameter that discounts future reward\n",
    "w1 = 1   # weight for health value\n",
    "w2 = 1   # weight for social value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best action to take at each iteration and the reward it generated\n",
    "\n",
    "The first return value is the action \\\n",
    "The second return value is the updated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_action(stateValues, index, w1, w2):\n",
    "    state = State(index)\n",
    "    if state == State.DEAD:\n",
    "        return (0, gamma * stateValues[index])\n",
    "    if state == State.W30:\n",
    "        valueRou = get_total_reward(State(index+1), w1, w2)\n",
    "        return (0, valueRou + gamma * stateValues[index+1])\n",
    "    if state == State.W120:\n",
    "        valueCai =  get_total_reward(State(index-1), w1, w2)\n",
    "        return (1, valueCai + gamma * stateValues[index-1])\n",
    "    valueCai = get_total_reward(State(index-1), w1, w2);\n",
    "    valueRou = get_total_reward(State(index+1), w1, w2);\n",
    "    if valueCai < valueRou:\n",
    "        return (0, valueRou + gamma * stateValues[index+1])\n",
    "    else:\n",
    "        return (1, valueCai + gamma * stateValues[index-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[  0. 100. 150. 150. 150. 150.  10.  10.  10.   0.   0.]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "2\n",
      "[  0. 175. 225. 225. 225. 225.  85.  15.  15.   5.   0.]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "3\n",
      "[  0.  212.5 262.5 262.5 262.5 262.5 122.5  52.5  17.5   7.5   2.5]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "4\n",
      "[  0.   231.25 281.25 281.25 281.25 281.25 141.25  71.25  36.25   8.75\n",
      "   3.75]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "5\n",
      "[  0.    240.625 290.625 290.625 290.625 290.625 150.625  80.625  45.625\n",
      "  18.125   4.375]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "6\n",
      "[  0.     245.3125 295.3125 295.3125 295.3125 295.3125 155.3125  85.3125\n",
      "  50.3125  22.8125   9.0625]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "7\n",
      "[  0.      247.65625 297.65625 297.65625 297.65625 297.65625 157.65625\n",
      "  87.65625  52.65625  25.15625  11.40625]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "8\n",
      "[  0.       248.828125 298.828125 298.828125 298.828125 298.828125\n",
      " 158.828125  88.828125  53.828125  26.328125  12.578125]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "9\n",
      "[  0.        249.4140625 299.4140625 299.4140625 299.4140625 299.4140625\n",
      " 159.4140625  89.4140625  54.4140625  26.9140625  13.1640625]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "10\n",
      "[  0.         249.70703125 299.70703125 299.70703125 299.70703125\n",
      " 299.70703125 159.70703125  89.70703125  54.70703125  27.20703125\n",
      "  13.45703125]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "11\n",
      "[  0.         249.85351562 299.85351562 299.85351562 299.85351562\n",
      " 299.85351562 159.85351562  89.85351562  54.85351562  27.35351562\n",
      "  13.60351562]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "12\n",
      "[  0.         249.92675781 299.92675781 299.92675781 299.92675781\n",
      " 299.92675781 159.92675781  89.92675781  54.92675781  27.42675781\n",
      "  13.67675781]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "13\n",
      "[  0.         249.96337891 299.96337891 299.96337891 299.96337891\n",
      " 299.96337891 159.96337891  89.96337891  54.96337891  27.46337891\n",
      "  13.71337891]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "14\n",
      "[  0.         249.98168945 299.98168945 299.98168945 299.98168945\n",
      " 299.98168945 159.98168945  89.98168945  54.98168945  27.48168945\n",
      "  13.73168945]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "15\n",
      "[  0.         249.99084473 299.99084473 299.99084473 299.99084473\n",
      " 299.99084473 159.99084473  89.99084473  54.99084473  27.49084473\n",
      "  13.74084473]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "16\n",
      "[  0.         249.99542236 299.99542236 299.99542236 299.99542236\n",
      " 299.99542236 159.99542236  89.99542236  54.99542236  27.49542236\n",
      "  13.74542236]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "17\n",
      "[  0.         249.99771118 299.99771118 299.99771118 299.99771118\n",
      " 299.99771118 159.99771118  89.99771118  54.99771118  27.49771118\n",
      "  13.74771118]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "18\n",
      "[  0.         249.99885559 299.99885559 299.99885559 299.99885559\n",
      " 299.99885559 159.99885559  89.99885559  54.99885559  27.49885559\n",
      "  13.74885559]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "19\n",
      "[  0.        249.9994278 299.9994278 299.9994278 299.9994278 299.9994278\n",
      " 159.9994278  89.9994278  54.9994278  27.4994278  13.7494278]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "20\n",
      "[  0.        249.9997139 299.9997139 299.9997139 299.9997139 299.9997139\n",
      " 159.9997139  89.9997139  54.9997139  27.4997139  13.7497139]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "21\n",
      "[  0.         249.99985695 299.99985695 299.99985695 299.99985695\n",
      " 299.99985695 159.99985695  89.99985695  54.99985695  27.49985695\n",
      "  13.74985695]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "22\n",
      "[  0.         249.99992847 299.99992847 299.99992847 299.99992847\n",
      " 299.99992847 159.99992847  89.99992847  54.99992847  27.49992847\n",
      "  13.74992847]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "23\n",
      "[  0.         249.99996424 299.99996424 299.99996424 299.99996424\n",
      " 299.99996424 159.99996424  89.99996424  54.99996424  27.49996424\n",
      "  13.74996424]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "24\n",
      "[  0.         249.99998212 299.99998212 299.99998212 299.99998212\n",
      " 299.99998212 159.99998212  89.99998212  54.99998212  27.49998212\n",
      "  13.74998212]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "25\n",
      "[  0.         249.99999106 299.99999106 299.99999106 299.99999106\n",
      " 299.99999106 159.99999106  89.99999106  54.99999106  27.49999106\n",
      "  13.74999106]\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define error functions\n",
    "def get_value_diff(oldValues, newValues):\n",
    "    err = 0.0\n",
    "    for index in range(len(oldValues)):\n",
    "        if index < len(newValues):\n",
    "            err = max(abs(newValues[index]-oldValues[index]), err)\n",
    "        else:\n",
    "            return 10000.0\n",
    "    return err\n",
    "            \n",
    "\n",
    "def do_value_iterations():\n",
    "    err = 10000.0 \n",
    "    iter_i = 0\n",
    "    stateValues = np.zeros(numberOfStates)\n",
    "    policy = np.zeros(numberOfStates)\n",
    "    while err > epsilon and iter_i < maxIter:\n",
    "        iter_i += 1\n",
    "        newStateValues = np.zeros(numberOfStates)\n",
    "        for index in range(numberOfStates):\n",
    "            (policy[index], newStateValues[index]) = find_best_action(stateValues, index, w1, w2)\n",
    "        err = get_value_diff(stateValues, newStateValues)\n",
    "        stateValues = newStateValues\n",
    "        print(iter_i)\n",
    "        print(stateValues)\n",
    "        print(policy)\n",
    "        \n",
    "do_value_iterations()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iterations (策略迭代）\n",
    "\n",
    "policy iteration has two main part\n",
    "1. Policy evaluation\n",
    "2. Pollcy improvement\n",
    "\n",
    "update_value_given_action is used for policy evaluation, in which the policy / action is known\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_value_given_action(index, action, stateValues, w1, w2):\n",
    "    if action == 0:\n",
    "        # rou\n",
    "        state = State(index)\n",
    "        if state == State.DEAD:\n",
    "            return stateValues[index] * gamma\n",
    "        if state == State.W120:\n",
    "            return 0\n",
    "        reward = get_total_reward(State(index+1), w1, w2)\n",
    "        return reward + stateValues[index+1] * gamma\n",
    "    else:\n",
    "        state = State(index)\n",
    "        if state == State.DEAD:\n",
    "            return stateValues[index] * gamma\n",
    "        if state == State.W30:\n",
    "            return 0\n",
    "        reward = get_total_reward(State(index-1), w1, w2)\n",
    "        return reward + stateValues[index-1] * gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, w1, w2):\n",
    "    err = 10000.0 \n",
    "    iter_i = 0\n",
    "    stateValues = np.zeros(numberOfStates)\n",
    "    while iter_i < maxIter and err > epsilon:\n",
    "        iter_i += 1\n",
    "        newValues = np.zeros(numberOfStates)\n",
    "        for index in range(numberOfStates):\n",
    "            action = policy[index]\n",
    "            newValues[index] = evaluate_value_given_action(index, action, stateValues, w1, w2)\n",
    "        err =  get_value_diff(newValues, stateValues)\n",
    "        stateValues = newValues\n",
    "    return stateValues "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(stateValues, policy):\n",
    "    newPolicy = np.zeros(len(stateValues))\n",
    "    for index in range(len(stateValues)):\n",
    "        state = State(index)\n",
    "        if state == State.DEAD:\n",
    "            newPolicy[index] = policy[index]\n",
    "        elif state == State.W30:\n",
    "            newPolicy[index] = 0 \n",
    "        elif state == State.W120:\n",
    "            newPolicy[index] = 1\n",
    "        else:\n",
    "            rewardCai = get_total_reward(State(index-1), w1, w2) + gamma * stateValues[index-1]\n",
    "            rewardRou = get_total_reward(State(index+1), w1, w2) + gamma * stateValues[index+1]\n",
    "            if rewardCai < rewardRou:\n",
    "                newPolicy[index] = 0\n",
    "            else:\n",
    "                newPolicy[index] = 1\n",
    "    return newPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main iterations that alternate between policy evaluation and police improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iterations():\n",
    "    err = 10000.0\n",
    "    iter_i = 0\n",
    "    policy = np.ones(numberOfStates)\n",
    "    while err > epsilon and iter_i < maxIter:\n",
    "        stateValues = policy_evaluation(policy, w1, w2)\n",
    "        print (\"iteration\", iter_i)\n",
    "        print(stateValues)\n",
    "        newPolicy = policy_improvement(stateValues, policy)\n",
    "        print(policy)\n",
    "        err = get_value_diff(policy, newPolicy)\n",
    "        policy = newPolicy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "[  0.       0.       0.     100.     200.     250.     135.      77.5\n",
      "  48.75    24.375   12.1875]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "iteration 0\n",
      "[  0.         249.99999106 299.99999106 299.99999106 299.99999106\n",
      " 299.99999106 159.99999106  89.99999106  54.99999106  27.49999106\n",
      "  13.74999106]\n",
      "[1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "policy_iterations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
